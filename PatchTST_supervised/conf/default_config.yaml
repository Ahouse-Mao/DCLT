# Default configuration for PatchTST training
# This file contains the default parameters that can be overridden in experiment-specific configs

# Random seed
random_seed: 2021

# Basic config
is_training: 1  # 1 for training, 0 for testing only
model_id: "test"
model: "PatchTST"  # model name, options: [Autoformer, Informer, Transformer, PatchTST]

# Data loader
data: "ETTh1"  # dataset type
root_path: "./dataset/"  # root path of the data file
data_path: "ETTh1.csv"  # data file
features: "M"  # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate
target: "OT"  # target feature in S or MS task
freq: "h"  # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h
checkpoints: "./checkpoints/"  # location of model checkpoints

# Forecasting task
seq_len: 336  # input sequence length
label_len: 48  # start token length
pred_len: 96  # prediction sequence length

# PatchTST specific parameters
fc_dropout: 0.05  # fully connected dropout
head_dropout: 0.0  # head dropout
patch_len: 16  # patch length
stride: 8  # stride
padding_patch: "end"  # None: None; end: padding on the end
revin: 1  # RevIN; True 1 False 0
affine: 0  # RevIN-affine; True 1 False 0
subtract_last: 0  # 0: subtract mean; 1: subtract last
decomposition: 0  # decomposition; True 1 False 0
kernel_size: 25  # decomposition-kernel
individual: 0  # individual head; True 1 False 0

# Transformer/Formers specific parameters
embed_type: 0  # 0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding
enc_in: 7  # encoder input size
dec_in: 7  # decoder input size
c_out: 7  # output size
d_model: 512  # dimension of model
n_heads: 8  # num of heads
e_layers: 2  # num of encoder layers
d_layers: 1  # num of decoder layers
d_ff: 2048  # dimension of fcn
moving_avg: 25  # window size of moving average
factor: 1  # attn factor
distil: true  # whether to use distilling in encoder
dropout: 0.05  # dropout
embed: "timeF"  # time features encoding, options:[timeF, fixed, learned]
activation: "gelu"  # activation
output_attention: false  # whether to output attention in encoder
do_predict: false  # whether to predict unseen future data

# Optimization
num_workers: 10  # data loader num workers
itr: 1  # experiments times
train_epochs: 100  # train epochs
batch_size: 128  # batch size of train input data
patience: 100  # early stopping patience
learning_rate: 0.0001  # optimizer learning rate
des: "Exp"  # exp description
loss: "mse"  # loss function
lradj: "type3"  # adjust learning rate
pct_start: 0.3  # pct_start
use_amp: false  # use automatic mixed precision training

# GPU
use_gpu: true  # use gpu
gpu: 0  # gpu
use_multi_gpu: false  # use multiple gpus
devices: "0,1,2,3"  # device ids of multiple gpus
test_flop: false  # See utils/tools for usage
